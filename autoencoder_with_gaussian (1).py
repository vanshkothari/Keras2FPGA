# -*- coding: utf-8 -*-
"""autoencoder_with_gaussian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wOxxFdldAmpPujZovN1jX4o5AmzGfJ7p
"""
# Source code for Keras model: https://github.com/jcbooth2/autoencoder_for_physical_layer/blob/master/autoencoder_with_gaussian.ipynb

# Import Libraries
import numpy as np
import tensorflow as tf
import keras
from keras.layers import Input, Dense, GaussianNoise,Lambda,Dropout, Concatenate
from keras.models import Model
from keras import regularizers
from keras.layers import BatchNormalization
from keras.optimizers import Adam,SGD
from keras import backend as K
# %matplotlib inline

# Set random seeds
from numpy.random import seed
seed(3)

np.random.seed(3)

# Set the defining parameters
# n = n_channel complex numbers (so 2n real numbers)
# k = log2(M), where M is the number of messages to encode
# EbNo is the energy per bit to noise power density

# Encoder Parameters
M = 16
k = np.log2(M)
n_channel = 1
R = k/n_channel
tx_power = 2
print('M:',M,'\t','n:',n_channel)

# Channel Parameters
EbNo=10.0**(7/10.0)
noise_std = np.sqrt(1/(2*R*EbNo))
num_taps = 1
reyleigh_std = num_taps/np.sqrt(2)

#generating data of size N
N = 16000
label = np.random.randint(M,size=N)

# creating one hot encoded vectors
train_data = []
for i in label:
    temp = np.zeros(M)
    temp[i] = 1
    train_data.append(temp)

# checking data shape
train_data = np.array(train_data)#, dtype = bool)
print (train_data.shape)


# checking generated data with it's label
temp_check = [17,23,45,67,89,96,72,250,350]
for i in temp_check:
    print(label[i],train_data[i])

# Average Power Normalization
def fixed_power_norm(x):
    P_avg = 0.04
    beta = K.sqrt(K.sum(K.square(x)))
    return x / beta

# Defined Autoencoder

# Transmitter Layers
input_signal = Input(shape=(M,))
encoded = Dense(M, activation='relu')(input_signal)
encoded1 = Dense(2*n_channel, activation='linear')(encoded)
encoded2 = BatchNormalization()(encoded1)
encoded3 = Lambda(lambda x: tx_power/np.sqrt(2)*K.l2_normalize(x,axis=-1))(encoded2)

# Gaussian Channel Layer
EbNo_train = 5.01187 #  coverted 7 db of EbNo
channel1 = GaussianNoise(np.sqrt(1/(2*R*EbNo_train)))(encoded3)

# Reciever Layer
decoded = Dense(M, activation='relu')(channel1)
decoded1 = Dense(M, activation='softmax')(decoded)
autoencoder = Model(input_signal, decoded1)
adam = Adam(lr=0.01)
sgd = SGD(lr=0.02)
autoencoder.compile(optimizer=adam, loss='categorical_crossentropy')

# printing summary of layers and it's trainable parameters
print (autoencoder.summary())

# traning auto encoder
autoencoder.fit(train_data, train_data,
                epochs=100,
                batch_size=1024)

# making encoder from full autoencoder
encoder = Model(input_signal, encoded3)

# making channel from full autoencoder
channel_input = Input(shape=(2*n_channel,))

chan = autoencoder.layers[-3](channel_input)
channel_layer = Model(channel_input,chan)

# making decoder from full autoencoder
encoded_input = Input(shape=(2*n_channel,))

deco = autoencoder.layers[-2](encoded_input)
deco = autoencoder.layers[-1](deco)
decoder = Model(encoded_input, deco)

# generating data for checking BER
N = 10000
test_label = np.random.randint(M,size=N)
test_data = []

for i in test_label:
    temp = np.zeros(M)
    temp[i] = 1
    test_data.append(temp)

test_data = np.array(test_data, dtype = np.float32)
np.save('encoder/tb_input_features', test_data)
#test_data = np.array(test_data, dtype = bool)
# for plotting learned consteallation diagram

scatter_plot = []
for i in range(0,M):
    temp = np.zeros(M)
    temp[i] = 1
    scatter_plot.append(encoder.predict(np.expand_dims(temp,axis=0)))
scatter_plot = 1.5/2*np.array(scatter_plot)
print (scatter_plot.shape)

# ploting constellation diagram
import matplotlib.pyplot as plt
scatter_plot = scatter_plot.reshape(M,2,1)
plt.scatter(scatter_plot[:,0],scatter_plot[:,1])
#plt.axis((-2.5,2.5,-2.5,2.5))
plt.grid()
plt.xlabel('I Axis')
plt.ylabel('Q Axis')
plt.show()

# Calculating BER from 0dB to 20dB SNR
EbNodB_range = list(np.arange(0,14+1,2))
ber = [None]*len(EbNodB_range)
for n in range(0,len(EbNodB_range)):
    EbNo=10.0**(EbNodB_range[n]/10.0)
    noise_std = np.sqrt(1/(2*R*EbNo))
    print(noise_std)
    noise_mean = 0
    no_errors = 0
    nn = N
    noise = noise_std*np.random.randn(nn,2*n_channel)
    encoded_signal = encoder.predict(test_data)
    final_signal = encoded_signal+noise
    pred_final_signal =  decoder.predict(final_signal)
    pred_output = np.argmax(pred_final_signal,axis=1)
    no_errors = (pred_output != test_label)
    no_errors =  no_errors.astype(int).sum()
    ber[n] = no_errors / nn
    print ('SNR:',EbNodB_range[n],'BER:',ber[n])
np.save('autoencoder/autoencoder.srcs/sim_1/new/expected_ber', ber)
# ploting BER curve
import matplotlib.pyplot as plt
from scipy import interpolate
plt.plot(EbNodB_range, ber, 'bo',label='Autoencoder(1,2)')
plt.yscale('log')
plt.xlabel('SNR Range')
plt.ylabel('Block Error Rate')
plt.grid()
plt.legend(loc='upper right',ncol = 1)
plt.show()

#=============HLS4ML=========================
import hls4ml
import plotting
#========================Encoder======================================
encoder_hls4ml = Model(input_signal, encoded2)   #exclude lambda layer

y_keras = encoder_hls4ml.predict(test_data)
np.save('encoder/tb_output_predictions', y_keras)

config_encoder = hls4ml.utils.config_from_keras_model(encoder_hls4ml, granularity='name')

config_encoder['Model']['ReuseFactor'] = 8
config_encoder['LayerName'][encoder_hls4ml.layers[0].name]['Precision']['result'] = 'ap_uint<1>'

hls_model_encoder = hls4ml.converters.convert_from_keras_model(
    encoder_hls4ml, hls_config=config_encoder, output_dir='encoder/encoder_hls', project_name = 'encoder', part='xc7a35tcpg236-1', backend='Vitis',
    input_data_tb = 'encoder/tb_input_features.npy',
    output_data_tb = 'encoder/tb_output_predictions.npy'    
)

hls4ml.utils.plot_model(hls_model_encoder, show_shapes=True, show_precision=True, to_file=None)

hls_model_encoder.compile()

X_test = np.ascontiguousarray(test_data)
y_hls = hls_model_encoder.predict(X_test)

hls_model_encoder.build()#csim=False)
#hls4ml.report.read_vivado_report('model_1/hls4ml_prj/')
#========================Decoder======================================
config_decoder = hls4ml.utils.config_from_keras_model(decoder, granularity='name')
config_decoder['Model']['ReuseFactor'] = 8


#generate input data for decoder
encoder_with_noise = Model(input_signal, channel1)

decoder_test_input = encoder_with_noise.predict(test_data)
np.save('decoder/tb_input_features', decoder_test_input)
decoder_test_output = decoder.predict(decoder_test_input)
np.save('decoder/tb_output_predictions', decoder_test_output)

hls_model_decoder = hls4ml.converters.convert_from_keras_model(
    decoder, hls_config=config_decoder, output_dir='decoder/decoder_hls', project_name = 'decoder', part='xc7a35tcpg236-1', backend='Vitis',
    input_data_tb = 'decoder/tb_input_features.npy',
    output_data_tb = 'decoder/tb_output_predictions.npy'    
)

hls_model_decoder.compile()
hls_model_decoder.build()
